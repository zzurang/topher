{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import re\n",
    "import gensim, logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Flatten, LSTM, Conv1D, MaxPooling1D, Embedding\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "W2V_MIN_COUNT = 2\n",
    "W2V_SKIPGRAM_WINDOW = 4\n",
    "W2V_DIM = 100\n",
    "INPUT_MAX_LEN = 3000\n",
    "\n",
    "TRAIN_EPOCH = 20\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_SPLIT_RATIO = 0.20\n",
    "\n",
    "p = re.compile(\"\\{(.*)\\}\")\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z]{2,100}')\n",
    "\n",
    "def extract_labels(raw):    \n",
    "    match = p.search(raw.replace('\\n',''))\n",
    "    if match:\n",
    "        kvs = [tuple(line.split(':')) for line in match.group(1).split(',')]\n",
    "        return {k.strip(): v.replace(\"'\",'').strip() for (k, v) in kvs}\n",
    "    return {}\n",
    "\n",
    "def tokenize_raw_sentence(sentence):\n",
    "    return [stemmer.stem(token) for token in tokenizer.tokenize(sentence) if token not in stopwords]\n",
    "\n",
    "def build_padded_sequences(w2v_model, tokenized_sentences):\n",
    "    index2word = w2v_model.wv.index2word\n",
    "    word2index = {w:i for i,w in enumerate(index2word)}\n",
    "    sequences = [[word2index[word] for word in words if word2index.has_key(word)] for words in tokenized_sentences]\n",
    "    return pad_sequences(sequences, maxlen=INPUT_MAX_LEN, dtype='int32', padding='post', truncating='post', value=0.)\n",
    "\n",
    "def build_embedding_layer(w2v_model):\n",
    "    return Embedding(len(w2v_model.wv.index2word), \n",
    "                     W2V_DIM,\n",
    "                     weights=[w2v_model.wv.syn0],\n",
    "                     input_length=INPUT_MAX_LEN,\n",
    "                     trainable=False)\n",
    "\n",
    "def build_conv_net(input_len, output_len, w2v_model):\n",
    "    embedding_layer = build_embedding_layer(w2v_model)\n",
    "    sequence_input = Input(shape=(input_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(35)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(output_len, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def load_model(name):\n",
    "    model_config = pickle.load(open(\"./trained_models/{}-Config\".format(name), \"rb\"))\n",
    "    model_weights = pickle.load(open(\"./trained_models/{}-Weights\".format(name), \"rb\"))\n",
    "    model = Model.from_config(model_config)\n",
    "    model.set_weights(model_weights)\n",
    "    return model\n",
    "\n",
    "def persist_models(clf_y_pairs):\n",
    "    for name, clf, y in clf_y_pairs:\n",
    "        pickle.dump(clf.get_config(), open(\"./trained_models/{}-Config\".format(name), \"wb\"))\n",
    "        pickle.dump(clf.get_weights(), open(\"./trained_models/{}-Weights\".format(name), \"wb\"))    \n",
    "        pickle.dump(y.columns, open(\"./trained_models/{}-Columns\".format(name), \"wb\"))    \n",
    "\n",
    "def persist_word2vec(w2v):\n",
    "    w2v.save(\"./trained_models/Word2Vector\")\n",
    "    \n",
    "def load_word2vec():\n",
    "    return Word2Vec.load(\"./trained_models/Word2Vector\")\n",
    "        \n",
    "conn = sqlite3.connect('../crawler/bugzilla_cralwer.py')\n",
    "df = pd.read_sql('select result from resultdb_bugzilla_with_labels_with_priority limit 10', con=conn)\n",
    "df['row'] = df['result'].map(lambda row: json.loads(row)) \n",
    "label_sets = df['row'].map(lambda row: extract_labels(row['labels'])) \n",
    "\n",
    "df['raw_sentence'] = df['row'].map(lambda row: row['body']) \n",
    "df['y_category'] = label_sets.map(lambda label_set: label_set['category'])\n",
    "df['y_component'] = label_sets.map(lambda label_set: label_set['component'])\n",
    "df['y_severity'] = df['row'].map(lambda row: row['bug_severity']) \n",
    "df['y_priority'] = df['row'].map(lambda row: row['priority']) \n",
    "df['y_cftype'] = df['row'].map(lambda row: row['cf_type']) \n",
    "df = df.drop(['result','row'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = df['raw_sentence'].map(lambda raw_sentence: tokenize_raw_sentence(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_sentences, min_count=W2V_MIN_COUNT, size=W2V_DIM, window=W2V_SKIPGRAM_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = build_padded_sequences(w2v_model, tokenized_sentences)\n",
    "y_category = pd.get_dummies(df['y_category'])\n",
    "y_component = pd.get_dummies(df['y_component'])\n",
    "y_severity = pd.get_dummies(df['y_severity'])\n",
    "y_priority = pd.get_dummies(df['y_priority'])\n",
    "y_cftype = pd.get_dummies(df['y_cftype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_clf = build_conv_net(INPUT_MAX_LEN, y_category.shape[1], w2v_model)\n",
    "component_clf = build_conv_net(INPUT_MAX_LEN, y_component.shape[1], w2v_model)\n",
    "severity_clf = build_conv_net(INPUT_MAX_LEN, y_severity.shape[1], w2v_model)\n",
    "priority_clf = build_conv_net(INPUT_MAX_LEN, y_priority.shape[1], w2v_model)\n",
    "cftype_clf = build_conv_net(INPUT_MAX_LEN, y_cftype.shape[1], w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_y_pairs = [\n",
    "    ('Category-Classifier', category_clf, y_category),\n",
    "    ('Component-Classifier', component_clf, y_component),\n",
    "    ('Severity-Classifier', severity_clf, y_severity),\n",
    "    ('Priority-Classifier', priority_clf, y_priority),\n",
    "    ('Cftype-Classifier', cftype_clf, y_cftype)    \n",
    "]\n",
    "\n",
    "for name, clf, y in clf_y_pairs:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y.as_matrix(), test_size=TRAIN_SPLIT_RATIO, random_state=42)\n",
    "    print 'Training {} Now...'.format(name)\n",
    "    clf.fit(x_train, y_train, batch_size=TRAIN_BATCH_SIZE, epochs=TRAIN_EPOCH, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Persisting models and word_embedding\n",
    "# persist_models(clf_y_pairs)    \n",
    "# persist_word2vec(w2v_model)\n",
    "# load_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inquery = \"\"\"\n",
    "    Bhavesh reported Astro becoming VERY slow for a system that had been up for 7+days\n",
    "\"\"\"\n",
    "converted_inquery = build_padded_sequences(w2v_model, [tokenize_raw_sentence(inquery)])\n",
    "\n",
    "for name, clf, y in clf_y_pairs:    \n",
    "    label_index = np.argmax(clf.predict(converted_inquery), axis=1)\n",
    "    label = y.columns.tolist()[label_index]\n",
    "    print '\"{}\" labled the input as \"{}\"'.format(name, label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
